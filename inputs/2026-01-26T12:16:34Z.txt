Transcript from2026-01-26T12:16:34Z
Speaker 1 00:00:08
Hey everyone, good morning, you know if he's able to join he's on another call with Shane right now, so he may be a few minutes late.
Speaker 2 00:00:22
We can kick off. I am recording, so we can send this over to him if he's not able to join. Thanks all for making the time. It's early Monday morning, relatively. We just wanted to get everyone in the same room, figuratively speaking, to talk Kade strategy. I've heard some conflicting pieces of info. And I think Jake and the Kade team are making a lot of great progress. So I want to make sure that folks are aware of that. Also that the clearinghouse team has what they need, relative to selling a business and communicating with DMV so that we're all speaking from the same language, and that we have an idea of kind of when to expect some of the Kade changes they're rolling out. So Jake, if you don't mind, can you kind of walk us through a high-level overview of kind of where you're at and the approach we've taken thus far? And then we can kind of open to questions.
Speaker 3 00:00:53
Yeah, absolutely. So the way we're approaching this is, obviously, we've talked about targeting NTR transactions. So it's a large percentage of clearinghouse volume. NTR is a significant portion of repose. And it looks like most of the standard path, we could call it, is essentially going to be security agreements, statement of reposesessions, and then POAs. With that in mind, what we started with is creating the piece for the document analysis. So we're trying to understand from the auto-decisioning level how best to approach looking at these documents, taking the info from them, getting them into a structured format, and then doing analysis to say whether or not they match the expected results of that.
Speaker 3 00:01:31
So we've gone down that path. We've looked at those three documents. We think we have a really good solution based on our test cases for that. I'll have a readout report for the POA piece tomorrow for that. But at this point, we think we have a really good path for document handling. So next steps after that, we're having an architecture review today to just kind of review what Nate, Justin, and Dustin and the team has been building in terms of a pipeline to put all that stuff together. So we need the order information. We need the VITAS data. We need the document analysis. So how to take all those pieces together and then from that, apply business rules and logic, which the Clearinghouse team I know is working on.
Speaker 3 00:02:04
And they're giving us the rules that essentially will indicate if something should be approved or rejected, or we should consider something from manual review, letting the DMV team know that they have to look at that because it is, I don't know, there's a gray area. There's edge cases or something like that. So the review today is to kind of like understand how that all works together and all those pieces fit together. And then our next target is to build that out and connect that to production data without DMV knowing about it. Now, the reason we're doing that is because we want to have a high level of confidence in what we're seeing before we let DMV kind of open up the curtain to DMV to see what's.
Speaker 3 00:02:36
going on. So once we connect to production data, what we're essentially looking at is NTR transactions and how Kade does against what we're seeing the decisions being made from the title clerks in VDOS production. So we're currently targeting that connection to production to be at the latest late December. So that would mean by some time in December, we're able to have Kade run the analysis, make a decision, and then be able to look at that against the decisions that were made by title clerks for NTR transactions in the DMV.
Speaker 3 00:03:09
some weeks to kind of verify our findings, right? Like how well did we do? Is it consistent? Is Kade maybe better in some ways than a title clerk? So for instance, that scenario might include, we would consider something rejection based on the evidence that we're given, but maybe a title clerk approved that. So it's all those different things that we want to be able to have the production data available to us in order to tweak Kade or to make adjustments or to make adjustments to business rules or modify the LLM agents or whatever it is. We're going to have that time to be able to make sure that we get it to the place where we want it to be before we let the.
Speaker 3 00:03:40
DMV see Kade doing the auto decisioning recommendations. Is there any questions so far or any questions about the approach or the expectations. 
Speaker 4 00:03:52
I have one, Jake. I know POA was added a little bit later into the deal because we didn't really identify that, but we're not looking at POAs from one state. You can ingest any POA, right? There's no standard POA that you're analyzing for, right. 
Speaker 3 00:04:08
Right. That's exactly correct. So we're looking at a bunch of different format variants. So the same principle would hold true for security agreements because there could be a large number of these types of documents, depending on if it's from a dealership or it's from a lender. So what we've been trying to prove out is that it shouldn't matter if it's a standardized document or not. Our agents should be able to handle various formats, a mix of handwritten signatures versus e-signatures, like all that stuff. So in order to prove that out...
Speaker 4 00:04:35
Yeah, the security agreement. Yeah, absolutely. I know that you were... We do have some standardized documents. There are some POAs that are West Virginia approved. I just want to make sure it was wider than that because we're not getting one specific type of POA.
Speaker 3 00:04:47
Yes. The answer is yes. Was there any other questions about our rollout approach? So hooking up to production first in what we're calling a shadow mode, where we're able to verify our findings against production data and then being able to potentially modify things before we would let DMV step into their pilot piece, where essentially they would just be looking at our recommendations and seeing whether or not they agree with those. By that point, we should have an alignment where we're confident that if we recommend something for approval or rejection, they would agree with that. So that's the goal of that second piece of the pilot where they're actually seeing our recommendations.
Speaker 4 00:05:26
I like it. Are you on track to the dates that we have showing on the DTR's meetings, which would have it, what, January would be the live in production? Is that right, Zivad. 
Speaker 1 00:05:39
Yeah, so January would be the DMV pilot. We have January 16th as that target date.
Speaker 3 00:05:46
Yeah, and that's just... Just to be clear, that hasn't been communicated externally, correct. 
Speaker 4 00:05:51
No, we haven't said anything to the DMV.
Speaker 3 00:05:53
Okay. So again, like these things are, it's important for us to have milestone dates, so that we're doing a time box proof of concept, right? Where we're not like, obviously we could take, let's say we had a year, we could take that entire year to do these things, but we don't want to do that. We want to be able to time box up and say, we think it's good enough, based on where we think the velocity is going for this POC to say like, that's reasonable, but it's flexible enough to change if we want it to. So I would say January is still a good goal to have for releasing this into production.
Speaker 3 00:06:26
in a way that they're seeing our recommendations. That could be. later january maybe but we're still as of now we're still targeting that until we find something that we would need to communicate out and say we think that's unreasonable and we would need more time so i think part of what tyler was also trying to get at with the communication piece is we want to be able to communicate that early so if let's say in early december we're realizing that this might not hit the target milestone dates that we're looking at we want to be able to communicate that out say okay maybe it'll be two weeks later than the initial target for that we want to be able to communicate that out early so there's no confusion with the expectations are still.
Speaker 3 00:06:57
lying before that.
Speaker 1 00:06:59
Yeah, I would agree with that, right? I mean, there is some time that the DMV will need to prep their clerks for the pilot as well. So I do want you guys to be comfortable and confident at a date. The sooner we can get to a point where we feel like it's good to share or a target date to share, that'd be good to have sooner than later.
Speaker 3 00:07:15
Yeah. And then, you know, with that, too, we're talking about, you know, a time where there's a lot of PTO, there's holidays, there's different things like that, right? So that may just extend some of the dates out just simply based on the fact that resources not being here for that. So I think that's something we will just look at. And the closer we get, the more easy it will be to target specific timelines like that. But I would say, at this point, I think we're still good for sometime mid or late January to be able to have DMV start to see some of these recommendations.
Speaker 4 00:07:43
So now I'm going to ask the question you're not going to like. And I only ask this because we have a couple of clients that their volume are game changers. But, you know, we decided to go as a team for the document analysis and do the MTR first, which is fine. So say we hit those dates, we're pretty confident beginning of February. And this probably isn't a fair question at this point, because I don't think you've gone through all the requirements, but I know you've seen it and I know you guys have debated it. But where does this get us? If that's working and you've got that part of CADE, you know, doing what it needs to do, the auto rejections on the front end, how difficult or how long of a timeline do you foresee getting to that?
Speaker 4 00:08:20
And again, the reason I ask is, and I'll just be transparent, Carvana is we're finally over a pretty big hurdle and they may be bringing some substantial volume and having some sort of auto rejection up front is going to be the only way that the state, they're not going to triple their team. So we're going to have to use CADE to help, you know, reject stuff out of the gate that we don't want a clerk ever looking at.
Speaker 5 00:08:40
So we talked about this last week, rejections, the default is manual review, right? We can only auto reject where we have clear evidence that it's rejected. Yeah. False negatives are. as damaging as false positives um if you look at what's happened today when we get rejections for the surprising rejections right the clerk does something weird system does something weird we just have one today reported by um Maggard uh it causes confusion a lot of turn a lot of manual effort to figure out what went wrong why was it wrong um so in the big scheme of things you know.
Speaker 5 00:09:14
if we find an ambiguous mismatch right like they provided the title of estate that doesn't match and then later we can auto reject that all day long um if there's a question of interpretation on whether a value is or isn't correct our default strategy is manual review not, preemptive auto reject so you know we're not far enough along to i think to, tell you all the things that we can auto reject on but it's going to be a limited set because, False negatives are damaging and our default process is manual review.
Speaker 4 00:09:47
Yeah, I agree with you. I just, I think it's our job on the business side to see how many are rejected for legitimate reasons that are straightforward, missing documentation, misalignment with what's clearly in the user guide, right? Like there, I get what you're saying. There's going to be some where we just can't, we can't put in that bucket yet. But I think there's enough where when we're talking about clean transactions on the, you know, the dealer use case, that's just where my mind goes. Because that's what we're going to get when we have, you know, if we get Carvana transacting and not that we're building for one, but their volume, the volume that they're committing.
Speaker 4 00:10:19
to is 40 to 60,000.
Speaker 5 00:10:21
So the message I got from Carvana from Josh when we talked to him last year was. It's not that they'll auto reject themselves. They'll send the data through their own human process. If their rules, which they want to be 100% aligned with the state's rules, aren't met, obviously, by the evidence and data that they have. So that's, that's what I heard from them is, they want us to be executing the same set of rules all the time, so that they can execute them first, so that they know whether or not they have a problem. And if they have a problem, they'll go to a human review process and do whatever is necessary prior to actually submitting to the state.
Speaker 5 00:10:56
Including maybe going to a different venue, where they think that will process.
Speaker 4 00:10:59
Agreed. I think you're spot on. I think what gave West Virginia more comfort is they were actually in their facility and saw that process. So I think we're all talking about the same, you know, from the same playbook. But the bigger, the bigger piece of that is mirroring that up and making sure that, I mean, we just can't, there's no way that West Virginia is going to hire people to meet that demand. It's going to have to be a level of automation that makes sure that that match you're talking about is exactly in alignment.
Speaker 6 00:11:25
Yes, I agree.
Speaker 3 00:11:28
Yeah. And so, again, just to be clear, when we're talking about rolling this out, we're talking about having those three buckets for the transaction, right? So it's not going to be rejections first and then manual reviews, and then maybe we'll get to approvals. What we're saying is that the building blocks we're creating right now should be able to support all three of those buckets, approve, reject, and manual review. The question is going to be, once we start running transactions through these, how big are those buckets, right? So in other words, and I'm just making numbers up for the sake of discussion. But if we find that 80% of the transactions are all going to manual review and only 20%
Speaker 3 00:12:03
are either approve or reject, are we comfortable with that? Is that good enough based on the volume? Is that something we will then be okay with releasing? Or would we then start to take steps to try to shrink that manual review down from 80% to something else that we feel is a better deliverable for the BMB in terms of being able to handle volumes quicker through automation. 
Speaker 5 00:12:24
And you were saying only 20% auto-approved, is that what you said. 
Speaker 3 00:12:26
Yeah, and I'm making these numbers up.
Speaker 5 00:12:28
Right, I hope you are. What I'm saying is... I mean, we want it to be flipped, right? So it's got to be more like 80% auto-approved. The MPR is a very standard transaction. We know the sources of variability, right? We have different security agreements, which so far the process has worked well on, including, identifying legal recourse. We have the POAs, which again, are not structured, but a couple of data points we got to pick off and confidence right now, I think is high. I don't know if those experiments have been run or about to be run.
Speaker 3 00:12:54
They've already been run. Yeah, they're in the 90% as well. So it's very similar to security agreements.
Speaker 5 00:12:59
Okay, so let's just say we're 90% of the 100%, right? So if we have 5% rejection, and we have 95% that should be approved, and we get 90% of those, right? We're 80% auto-approved, which means we have a 20% burden on manual. Right. So if we were running 10 clerks for full auto-approved, we'd only need two.
Speaker 3 00:13:19
Right. And that's what we're trying to get to, is what I'm saying. So once we hook up into production, and we can kind of see how big of a percentage each recommendation bucket is, then we can have better confidence to know, wow, okay, we can handle a bunch of these transactions. What I was saying is, hooking us up to production on that chat mode gives us the ability to potentially work through any issues we might have if we feel manual review is too big. So then we have time to modify things in order to shrink that down. So we're moving into more approvals and rejections of that.
Speaker 4 00:13:51
Yeah, I think to put it into and set of percentages and numbers, like where their staff today and how their staff, when we get to 15,000 a month, we're done. I don't see any way that they're going to be able to process, you know, with the rejection rate we have today, right, which is between eight and 10%. And to keep cycle times around seven to nine hours. All those are factors, right? Like we could let the cycle time go up and they could do, you know, potentially we could see something different. We could get rejection rate down, which we are. That's what Sarah and Zabet are working really hard on. And they've done a really good job with alignment of the clerk's user guides to the client's user guide, so on and so forth, and doing individual meetings with clients who have high rejection rates because they're just not following the process. The problem is every time we bring on a new client, you're going to see it spike, right? Because they always, they tend to need four to six weeks to really iron out and teach their clients or teach their people, whoever, however they're submitting. So it's not a, it's not a problem that just disappears.
Speaker 4 00:14:43
But overall, I'm happy. Like if we can, if we can get this moving to that. Timeline, I think that we can. you know, we can be in the realm of not getting too far behind if some of the newer opportunities come through at the timelines we think they are. So yeah, I have just one input on two with the.
Speaker 7 00:15:04
staffing. So the rules that we are currently building out on a field level, we are going to base that off the supervisors and not the clerks themselves. Because obviously, the feedback when the team went to West Virginia is that supervisors and clerks do base their decisions on very different things, and it's not really consistent. So I just kind of wanted to talk about how we are going to account for that during the shadowing, for example. If we have the supervisor's rules, that would reject something, but a clerk approves it, like how are we going to handle those kind of discrepancies? The same thing for when this go live, mid-January, end of January, whenever it is.
Speaker 7 00:15:38
Is it clerks or supervisors who is going to do the reviews on the K-data decisioning. 
Speaker 3 00:15:43
Well, so I guess to answer your first point, what we would like to come back, with for the initial stage of the pilot where we're just looking to production but they're not seeing our recommendations like we need to come back with data that shows the reason we rejected this is because of your policy now and your title clerk approved this transaction but we don't think they should have and here's why and we give them all the reasons why that actually should have been rejected that that's that's one thing so the second piece where who's going to be looking at the cue of the recommendations is we've talked about creating a group of users that we feel are.
Speaker 3 00:16:15
making the decisions for the transactions based on the policy and not simply like title clerk preferences so the people we would expect to look at the cue see our recommendation and then approve or reject at that point is a select group of users that we feel are aligning with the policies stated by the dmv and not just simply like title preferences okay right so let me let me clarify.
Speaker 5 00:16:33
that a little bit further that requires a new group within their identity provider that will be like, Clearinghouse CAID reviewers or something, right? That's work that West Virginia will have to do to send it, to add that value to the particular users and then send it to us, which will happen automatically. Then we have to do a little bit in our key code to map that or interpret that on the code on our side. So we don't want to limit the access to existing groups of supervisors or Clearinghouse or whatever. We want to identify a new group and allow them to specifically target the people they want to target,
Speaker 5 00:17:03
whether they're supervisors currently or not, and have them operate. That's the most flexibility for a little bit of extra work.
Speaker 2 00:17:15
Sweet, quick time check. Brian did have to drop. We're right at 11. Anything else you want to interview today? Cool. Conversations seem productive. Hopefully this is helpful getting everybody back in sync. Like I said, I did record this. I will post this in SharePoint and share the link somewhere in Slack. And yeah, that's it. Thanks all for the time. Appreciate it. Thanks, everybody.
Speaker 6 00:17:34
Thanks, everyone. Thank you.
Summary: 
> Date: 2026-01-26 12:16:34
> Location: [Insert Location]
> Participants: [Speaker 1] [Speaker 2] [Speaker 3] [Speaker 4] [Speaker 5] [Speaker 6] [Speaker 7]
## Meeting Notes
### Kade Project Strategy and Timeline
*   **Discussion**: The team reviewed the current strategy for the Kade project, which automates transaction processing for the DMV.
    *   The initial focus is on NTR (New to Reporter) transactions, specifically targeting three key documents: security agreements, statements of repossessions, and POAs (Power of Attorney).
    *   A document analysis solution has been developed and is producing positive test results. A readout report for the POA component is expected tomorrow.
    *   An architecture review is scheduled today to discuss the pipeline for integrating order information, VITAS data, and document analysis.
    *   The Clearinghouse team is providing business rules to help Kade approve, reject, or flag items for manual review.
*   **Conclusion**: The next step is to connect Kade to production data in a shadow mode by late December. This will enable comparison of Kade’s decisions against those made by human title clerks without the DMV seeing the results, allowing further refinement.
### Kade’s Document Handling Capabilities
*   **Discussion**: A question was raised about whether Kade can handle various POA formats, not just a standardized one.
    *   The system is designed to handle a wide variety of document formats, including different POAs and security agreements from various lenders or dealerships.
    *   The goal is to demonstrate that Kade’s agents can process non-standardized documents with mixed formats such as handwritten and e-signatures.
    *   Tests on various POA formats have been successful, with results in the 90% accuracy range, similar to security agreements.
### Rollout Plan and Pilot Phase
*   **Discussion**: The team discussed the timeline for the DMV pilot and communication strategy.
    *   The internal target date for the DMV pilot—when they would see Kade’s recommendations—is January 16th. This date has not been communicated to the DMV yet.
    *   The team agreed that January 16th is a good goal but remains flexible. Any potential delays will be communicated early.
    *   Timeline considerations include holiday PTO, which could extend dates.
    *   The DMV will need time to prepare their clerks for the pilot, so a confident target date is needed soon.
### Auto-Rejection and Manual Review Strategy
*   **Discussion**: The topic of auto-rejection was raised, particularly in light of potential high-volume clients like Carvana.
    *   The default strategy for ambiguous cases is manual review, not auto-rejection, to avoid false negatives that cause confusion and manual effort.
    *   Auto-rejections will be limited to clear-cut cases (e.g., a title from a mismatched state).
    *   The team is creating three decision buckets: approve, reject, and manual review.
    *   The goal is for a high percentage of transactions (e.g., 80%) to be auto-approved, with a much smaller portion requiring manual review. Shadow mode will help determine the size of these buckets.
    *   Carvana’s process runs transactions through their own internal human review first, based on state rules, before submitting them.
### Pilot User Group and Rule Consistency
*   **Discussion**: The team addressed how to handle discrepancies between clerk and supervisor decisions during the pilot.
    *   Kade’s business rules are based on supervisor standards, not clerk decisions, as supervisors are more consistent.
    *   During shadow mode, the team will analyze cases where Kade rejects a transaction that a clerk approved to highlight policy discrepancies.
    *   For the live pilot, a new user group of “Kade reviewers” will be created—select individuals aligned with DMV policy, regardless of current roles.
    *   This requires technical work from West Virginia to create the new group in their identity provider and corresponding mapping on the project’s side.
## Next Arrangements
-   [ ] Confirm and communicate the DMV pilot target date (currently January 16th) to allow DMV clerk preparation.
-   [ ] Complete architecture review and finalize integration plans for order information, VITAS data, and document analysis.
-   [ ] Begin shadow mode by late December and set up comparison reporting against human title clerk decisions.
-   [ ] Define and document decision buckets (approve, reject, manual review) and thresholds for auto-approval and auto-rejection.
-   [ ] Coordinate with the Clearinghouse team to finalize and validate business rules.
-   [ ] Engage with West Virginia DMV to create the “Kade reviewers” group in their identity provider and implement mapping on the project side.
-   [ ] Assess staffing and cycle-time contingencies if automation rates fall below targets or rollout is delayed.
## AI Suggestions
> **AI Suggestions**
> AI has identified the following issues that were not concluded in the meeting or lack clear action items; please pay attention:
> 1. The meeting confirmed a target date of January 16th for the DMV pilot but acknowledged it is flexible and has not been shared with the DMV. A firm date for communication with the DMV needs to be established to allow them adequate preparation time.
> 2. The strategy for managing large volume increases from clients like Carvana relies heavily on the success of Kade's automation. The impact on current staffing and cycle times if Kade's rollout is delayed or its auto-approval rate is lower than the desired 80% was discussed but no contingency plan was defined.
> 3. A new user group of "Kade reviewers" needs to be created by the West Virginia DMV for the pilot. The specific steps, timeline, and responsible parties for coordinating this technical setup with the DMV were not assigned.